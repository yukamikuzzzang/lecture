(1)
실습의 관련된 문제..
중간고사의 난이도는 낮추는 방향으로
(변별력을 떨어뜨릴 것.)
기말에 실습에서 따질 예정.

다항회귀..
2차로 회귀분석을 해도 1차를 찾아내더라.
2차 > 계수가 0으로 수렴.
3차로도 > 2차/ 1차를 찾을 수 있음.
n차라면 n차보다 작은 모양을 모두 커버할 수 있음.
(10차 이상이라면 힘들다고..)

x1 = x
x2 = x ** 2
x3 = x ** 3
...
x9 = x ** 9 
x10 = x ** 10
xa = np_c[x1,x2,...,x10]

급수:
데이터를 함수의 조합으로 보는 것.
선형을 좋아하는 수학자들.

정말로 모든 선형이 가능할까..?
(그건 아님.)

(2)
지능의 본질은 예측..?

예측:분류(Classfication),회귀

분류<끊어지는>:범주형:
사진을 보면 이게 사람/개/고양이?
회귀<연속적인>:연속형:
데이터가 들어왔을 때 그 예측치를 분석. (수치..!)

신경망
으로 회귀분석을 한다?

y값이 불연속적인 데이터는 어떻게 할 수 있을까..
기존의 방법으론 안되는데..

값을 예측하는 것이 아닌
1이 될 확률이 높아진 것이다.
(현상을 일으키는 수학적인 모델..)

확률예측을 위해 사용하는 로지스틱 함수
신경망에서도 되게 많이 등장한다고..

P(y=1) = f(x)

f(x)의 범위를 정하는 순간 의미가 없어진다.
P와 관련된 다른 함수를 만들어보자.

발생할 확률 : 발생하지 않을 확률 (의 비율)..

(오즈라는 이름의 함수)
P/(1-P) >> -로 가지 않는다.

log(P/(1-P))
log는 0에 가까워질 수록 -무한대로 떨어진다고
(승값을 뽑는 친구이기에)

로지스틱함수(=시그모이드 함수)
선형함수를 확률함수로 만드는 힘.
원리는 비슷..

(3)
모델 생성
sklearn.linear_model

regr.predict

0.5보다 크면 1, 아님 0..
x에 대한 예측 값을 판명.

(np.exp 부분)
2의 -f(x)승이기 때문에
계수에 -1을 곱함.
?? 왜 안나오누..
로지스틱 with numpy를 검색해보아야할듯..

다음 시간은 pandas..

